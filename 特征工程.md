### 特征选择方法都有哪些
- **无监督式方法**：
1、去掉方差很小的特征，如果一个特征取值在不同样本中没什么变化，那可以认为它的贡献度就比较小，可以考虑删除；
- **有监督式方法**：
sklearn中有基于模型的特征选择系列API
1、带L1,L2的回归分析，学习出来的参数趋于0的说明和目标相关程度低，可以考虑删除；
2、随机森林/XGBoost的特征重要性排序；
3、LDA (线性判别分析)
4、单变量分析(分析每个独立特征和响应目标的关系)，得分低的可以考虑删除；
5、顶层特征选择方法：选择一个模型，在不同特征子集上训练，选择得分最好的那个子集；

### 随机森林如何进行特征选择？
- 1、平均不纯度减少(MDI)
表示每个特征对误差的平均减少程度。
- 2、平均精确率减少(MDA)
利用OOB数据计算出基本误差，然后对每个特征，随机打乱顺序，再次计算误差。对于不重要的特征来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的特征来说，打乱顺序就会降低模型的精确率。
- 3、Boruta
(1) 对特征矩阵X的各个特征取值进行shuffle, 将shuffle后的特征(shadow features)与原特征(real features)拼接构成新的特征矩阵。
(2) 使用新特征矩阵作为输入, 训练可以输出feature_importance的模型。
(3) 计算real feature和shadow feature的Z_score。
(4) 在shadow features中找出最大的Z_score记为$Z_{max}$。
(5) 将Z_socre大于$Z_{max}$的real feature标记为"重要"，将Z_score显著小于$Z_{max}$的real feature标记为"不重要"，并且从特征集合中永久剔除。

重复1~5步骤，直到所有特征都被划分。

$Z_{score}=average\_oob\_error / oob\_error$的标准差。