##　传统机器学习

### １、模型评价标准有哪些？*

(1) 均方误差：
$$
E(f;D)=\frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^2
$$
(2) 精度：
$$
E(f;D)=\frac{1}{m}\sum_{i=1}^{m}\Pi(f(x_i)=y_i)
$$
(3) 查准率(Precision)与查全率/召回率(Recall)：
$$
P=\frac{TP}{TP+FP}
$$

$$
R=\frac{TP}{TP+FN}
$$

(4) F1：
$$
\frac{1}{F1}=\frac{1}{2}(\frac{1}{P}+\frac{1}{R})
$$


(5) AUC：

$m^{+}$个正样本和$m^{-}$个负样本，令$D^{+}$和$D^{-}$分别表示正负样本集合，则排序“损失”可以定义为：
$$
l_{rank}=\frac{1}{m^{+}m^{-}}\sum_{x^{+}\in D^{+}}\sum_{x^{-}\in D^{-}}(\Pi(f(x^{+})<f(x^{-}))+\frac{1}{2}\Pi(f(x^{+})=f(x^{-})))
$$

$$
AUC=1-l_{rank}
$$

AUC是ROC曲线下面的面积，衡量的是模型的排序性能，即正样本的预测值是不是更多的要大于负样本的预测值。

(6) logloss：
$$
L(Y,P(Y|X))=-log(P(Y|X))\left\{\begin{array}{cc} 
1, & x=f(Pa_{x})\\ 
0, & other\ values 
\end{array}\right.
$$
因为
$$
P(Y|X)= 
\left \{\begin{array}{cc}
f(x_i), &Y = 1\\
1-f(x_i), &Y = 0
\end{array}\right.
$$
所以最终logloss的形式表示为：
$$
L(f;D)=\sum_{i=1}^{m}[-y_ilog(f(x_i))-(1-y_i)log(1-f(x_i))]
$$

### 2、P和R怎么计算？F1怎么计算？*

见1的(3)(4)

### 3、ROC的横坐标和纵坐标分别是什么？*

TPR(正阳性率)和FPR(假阳性率)
$$
TPR=\frac{TP}{TP+FN}
$$

$$
FPR=\frac{FP}{TN+FP}
$$

前者是越大越好，后者是越小越好。

 ### 4、AUC是什么？画ROC曲线****

AUC定义见1的(5)

**画ROC曲线的技巧：从大到小设置阈值，大于该阈值的预测为正样本，小于该阈值的预测为负样本。然后根据3的公式算出TPR和FPR，描点。**

两根ROC曲线，能完全包住另一根的曲线对应的模型有更好的性能。但如果两根ROC曲线是交叉的，则得看具体应用场景，如果应用场景对假阳性率容忍度比较低则得选择特定范围内更靠左的曲线。如果没特殊要求则可以比较ROC曲线下的面积大小。

ROC曲线的优点：受类别不平衡的影响很小，而PR曲线受类别不平衡较大，见下图。

<img src="2.jpg" width="600px" align=center/>

### 5、样本不均衡是否会对AUC造成影响？*

不会，auc衡量的是模型的排序性能，即把正样本排到负样本前面的性能，与正负样本比例无关。

### 6、知道哪些损失函数？*

<img src="loss.png" width="500px" align=center/>

分类损失函数(为了方便表示均列出单样本的情况)：

(1) 0-1损失函数-蓝色
$$
L(y,f(x))=\left\{\begin{array}{cc} 
1,& if\ \ yf(x)>=0\\ 
0,& if\ \ yf(x)<0 
\end{array}\right.
$$
缺点：对误分类的点一视同仁，函数不连续且非凸，不容易优化。

(2) 对数损失/交叉熵损失(cross entropy)-绿色
$$
L(y,f(x))=-ylog(f(x))-(1-y)log(1-f(x))
$$
在二分类问题中logloss和交叉熵损失是等价的。

(3) 合页损失函数(hingle loss)-红色
$$
L(y,f(x))=max(0,1-yf(x))
$$
svm的等价损失函数，hinge loss使得$yf(x)>1$的样本损失均为0，由此带来了稀疏解，使得svm仅通过少量的支持向量就能确定超平面。

(4) 指数损失-紫色
$$
L(y,f(x))=e^{-yf(x)}
$$
Adaboost的等价损失函数，缺点是对异常点过于敏感，鲁棒性不强。

(5) Huber损失-橘黄色
$$
L(y,f(x))=\left\{\begin{array}{cc} 
max(0,1-yf(x))^2,& if\ \ yf(x)>=-1\\ 
-4yf(x),& if\ \ yf(x)<-1 
\end{array}\right.
$$
结合了hinge loss和logistic loss的优点，既能在$yf(x)>-1$时产生稀疏解提高训练效率，又能进行概率估计。另外对于$yf(x)<-1$样本的惩罚以线性增加，意味着降低了异常点的代价，鲁棒性有所提高。

<img src="rloss.png" width="500px" align=center/>

回归损失函数：

(1) 均方误差函数(MSE)
$$
L(y,f(x))=(y-f(x))^2
$$
最常用的回归损失函数，其缺点是异常点的代价比较高，鲁棒性不强；如果样本中有较多异常点，则绝对值损失表现更好一些。

(2) 绝对值损失
$$
L(y,f(x))=|y-f(x)|
$$
(3) Huber损失
$$
L(y,f(x))=\left\{\begin{array}{cc} 
\frac{1}{2}[y-f(x)]^2,& if\ \ yf(x)<=\delta\\ 
\delta|y-f(x)|-\frac{1}{2}\delta^2,& if\ \ yf(x)>\delta 
\end{array}\right.
$$
Huber损失是对二者的综合，当$|y-f(x)|$小于一个事先指定的值$\delta$时，变为平方损失，大于$\delta$时，则变成类似于绝对值损失，因此也是比较鲁棒的损失函数。

### 7、分类为什么用logloss？*

logloss的形式如下。在逻辑回归中，最终使用了sigmoid函数作为激活函数。把$f(x_i)=\dfrac{e^{\beta^Tx_i}}{1+\beta^Tx_i}$代入(1)式后可以得到损失函数的最终形式(2)：
$$
L(f;D)=\sum_{i=1}^{m}[-y_ilog(f(x_i))-(1-y_i)log(1-f(x_i))]\tag{1}
$$

$$
L(\beta)=\sum_{i=1}^{m}(-y_i\beta^{T}x_i+ln(1+e^{\beta^Tx_i}))\tag{2}
$$

该损失函数为凸函数，可以通过梯度下降来求得最优解。而把$f(x_i)$代入MSE损失得到的最终形式不是凸函数，此时存在损失函数多个局部极小值，影响梯度下降求全局最优解。

### 8、Logloss和AUC的区别*

1、logloss评估的是模型输出与真实值之间的偏离情况，对预测概率敏感；而auc评估的是的排序能力，对排序敏感而对预测分数不敏感；

2、logloss不仅可以作为模型的评估标准，还能作为模型的目标函数(交叉熵损失)；auc因为不可导，所以无法作为目标函数。

### 9、交叉验证的原理和作用？与直接划分训练测试集想比的优点？*

原理：

以k折交叉验证为例，把数据随机划分为k份，进行k次训练-测试。这k份样本轮流取其中一份作为测试集，另外k-1份作为该轮的训练集。取这k次结果的平均值作为最终结果。

优点：

在数据集规模较小时，划分出的测试集规模也将很小，一个小规模的测试集意味着平均测试误差估计的统计不确定性。这时候我们很难通过一次训练-测试来判断模型之间的性能差异。通过k折交叉验证可以充分利用所有数据进行训练-测试，得到的结果在统计意义上也更加的科学。

### 10、梯度下降法，牛顿法，拟牛顿法的区别？***

### 11、为什么梯度是函数变化最快的方向？*

### 12、机器学习里面常见的激活函数有哪些？为什么通常需要0均值？*

(1)sigmoid函数

<img src="sigmoid.jpeg" width="400px" align=center/>

sigmoid函数能将输出归一化到[0,1]之间，适合输出概率的情况；

缺点：

a、导数范围是[0, 0.25]，在接近函数饱和区域后网络的反向传播容易梯度消失。

b、sigmoid函数不以0为中心值，它的输出总是正数；那么关于![w](http://www.zhihu.com/equation?tex=w)的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这将会导致梯度下降权重更新时出现z字型的下降。

简单推导：
$$
f=\sum_i w_i x_i+b
$$

$$
\dfrac{df}{dw_i}=x_i
$$

$$
\dfrac{dL}{dw_i}=\dfrac{dL}{df}\dfrac{df}{dw_i}=\dfrac{dL}{df}x_i
$$

因为$x_i>0$，所以$\dfrac{dL}{dw_i}$的始终和$\dfrac{dL}{df}$同号。

c、计算比较耗时。

(2)tanh函数

<img src="tanh.jpeg" width="400px" align=center/>

tanh函数可以表示为：$tanh(x)=2sigmoid(2x)-1$，相对于sigmoid函数的缩放形式，它解决了sigmoid函数不以0为中心的缺点。但它的导数范围仍然在[0,1]之间，在深层网络的反向传播中仍然会导致梯度消失的问题。

(3)relu函数

<img src="relu.jpeg" width="400px" align=center/>

relu函数的形式非常简单，计算速度也远快于前两者，可以大大加快神经网络的收敛过程。而且当输出大于0的时候relu函数的导数为常数，这个时候不会产生梯度消失的现象。

缺点：

a、但容易看出，当输出落在relu函数的左半部分则输出为0，该神经元不会参与训练过程，相当于神经元死亡”。如果学习率设置过高会造成大量的“神经元死亡”。

b、没有完全解决梯度消失的问题。

(4)leaky relu函数

<img src="leaky.jpg" width="400px" align=center/>

leaky relu函数解决了relu函数的“神经元死亡”问题，其中$\varepsilon$是一个比较小的系数。

(5)maxout函数
$$
\max(w_1^Tx+b_1, w_2^Tx + b_2)
$$
解决了relu函数的“神经元死亡”问题，但增加了参数数量，相当于增加了一层网络。

### 13、KNN*

### 14、KNN复杂度高怎么解决*

### 15、LR和线性回归的区别和联系*

1、在逻辑回归中，我们试图建立不同类别的概率与自变量的联系，解决的是分类问题；而线性回归则是试图建立因变量和自变量之间的线性关系，解决的是回归问题。

2、从数据上来看，线性回归的因变量是连续的，符合正态分布；而逻辑回归的因变量是离散的，符合二项分布。

3、在训练模型时，线性回归可以采用解析解(最小二乘法)也可以采用梯度下降；而逻辑回归并不存在解析解，只能通过梯度下降训练模型。但这两种方法的本质都是极大似然估计。

### 16、LR推导\*\*\*\*\*\*\*\*\*\*\*\*

​        LR模型是由极大似然法推导得出的分类模型。对于一个线性模型，我们一般用y=w^T+b来表示。当一个样本点落在LR超平面上时有$P(y=1|x)=P(Y=0|x)=1-P(y=1|x)$，即$ln\dfrac{P(y=1|x)}{1-P(y=1|x)}=0=w^T+b$。整理可得：
$$
P(y=1|x)=\dfrac{e^{w^Tx+b}}{1+e^{w^Tx+b}}=\dfrac{1}{1+e^{-(w^Tx+b)}}\tag{1}
$$
这便是LR模型的函数表示，也是sigmoid函数的原型。

​	对于一个二分类问题，我们假设$P(y=1|x)=f(x)$为模型的函数表示，代表分类结果为1的概率；那么$P(Y=0|x)=1-f(x)$则表示分类结果为0的概率。根据极大似然法可以写出总的样本分布概率式子：
$$
\begin{eqnarray}
P(Y|X)&=&\prod_{i}P(y_i|x_i)^{y_i}(1-P(y_i|x_i))^{1-y_i}\\
&=&\prod_if(x_i)^{y_i}(1-f(x_i))^{1-y_i}
\end{eqnarray}
$$
对式子两边同时取对数可得：
$$
logP(Y|X)=\sum_i[y_ilogf(x_i)+(1-y_i)log(1-f(x_i))]\tag{2}
$$
为了方便推导，把累加号省略，把(1)式代入(2)可以得到：
$$
\begin{eqnarray}
L(w)&=&[ylogf(x)+(1-y)log(1-f(x))]\\
&=&[ylog\dfrac{e^{w^Tx+b}}{1+e^{w^Tx+b}}+(1-y)log\dfrac{1}{1+e^{w^Tx+b}}]\\
&=&[yloge^{w^Tx+b}-ylog(1+e^{w^Tx+b})-(1-y)log(1+e^{w^Tx+b})]\\
&=&y(w^Tx+b)-log(1+e^{w^Tx+b})
\end{eqnarray}
$$
因为极大似然函数需要求最大值，所以损失函数需要在前面加上负号，故LR模型的最终目标函数为：
$$
L(w)=\frac{1}{m}\sum_{i=1}^{m}[log(1+e^{w^Tx+b})-y(w^Tx+b)]
$$
LR模型的优化过程：

首先写一下sigmoid函数的性质：
$$
g(z)=\dfrac{1}{1+e^{-z}}\\
\dfrac{\partial g}{\partial z}=g(z)(1-g(z))
$$
显然有f(x)=g(w^Tx+b)由上面的推导可以知道LR模型的损失函数为：
$$
L(y,f)=ylogf(x)+(1-y)log(1-f(x))
$$
对w进行求导可得：
$$
\dfrac{\partial L}{\partial w}=\dfrac{\partial L}{\partial f}\dfrac{\partial f}{\partial z}\dfrac{\partial z}{\partial w}=\dfrac{y}{f(x)}\cdot f(x)(1-f(x))\cdot x^T+(1-y)\cdot \dfrac{1}{1-f(x)}\cdot(-f(x)(1-f(x)))\cdot x^T
$$
整理可得：
$$
\dfrac{\partial L}{\partial w}=(y^{'}-y)\cdot x^T
$$
其中$y^{'}$是真实标记，$y$是函数的输出值。

根据梯度下降的迭代公式有：
$$
w=w-\dfrac{\partial L}{\partial w}=w-(y^{'}-y)\cdot x^T
$$
这便是LR模型的优化过程。

### 17、LR如何解决共线性问题，为什么深度学习不强调？*

解决方法：

(1) 增加训练样本数量；

(2) 主成分分析PCA，提取主要的特征；

(3) 加入L1, L2正则化项；

(4) 逐步回归法：特征逐步加入回归模型，用已选入的特征进行F检验，如果原有的变量因为后引入的变量而变得不显著，就将其删除。

深度学习模型复杂度高，而且有很多正则化的trick，像L1,L2正则化，dropout，batch normalization等等；本身就具有选择特征、抽象深层特征的能力，所以可以解决共线性问题。

### 18、LR为什么使用sigmoid函数？*

### 19、LR的缺点*

(1)对共线性比较敏感，如果相关属性较多，将会削弱独立属性，造成模型有偏。需要利用因子分析或者变量聚类分析等手段来选择代表性的特征，以减少自变量之间的相关性；

(2)预测结果呈现S型，两端概率变化小，中间概率变化大比较敏感，导致很多区间的变量的变化对目标概率的影响没有区分度，无法确定阈值。

(3)模型比较简单，容易欠拟合，不能处理缺失特征等。

### 20、LR损失函数***

交叉熵损失的形式：
$$
L(f;D)=\sum_{i=1}^{m}[-y_ilog(f(x_i))-(1-y_i)log(1-f(x_i))]\tag{1}
$$
LR模型的函数形式：
$$
f(y=1|x)=\dfrac{e^{-(w^Tx+b)}}{1+e^{-(w^Tx+b)}}\tag{2}
$$
把(2)代入(1)可以得到：
$$
\begin{eqnarray}
L(f;D)&=&\sum[-ylog(\dfrac{e^{-(w^Tx+b)}}{1+e^{-(w^Tx+b)}})-(1-y)log(\dfrac{1}{1+e^{-(w^Tx+b)}})]\\
&=&\sum[-ylog(e^{-(w^Tx+b)})+ylog(1+e^{-(w^Tx+b)})+log(1+e^{-(w^Tx+b)})-ylog(1+e^{-(w^Tx+b)})]\\
&=&\sum-ye^{-(w^Tx+b)}+log(1+e^{-(w^Tx+b)})
\end{eqnarray}
$$
这便是LR模型损失函数的最终形式。

### 21、LR和最大熵模型的相同点和不同点*

### 22、如果所有样本都是正样本，那么训练出的LR模型的超平面是什么样的？*

所有样本点都在超平面的一侧。

### 23、讲一下softmax，需要推导***

### 24、推导softmax的梯度，和tanh的梯度(求导)*

### 25、softmax的损失函数*

同样是交叉熵函数的形式，只不过从LR的二元变成了多元：
$$
L=-\sum_{j=1}^{T}y_jlog(s_j)=-\frac{1}{m}\sum_{i=1}^{m}\sum_{j=1}^{k}y_i^jlog\dfrac{e^{\theta_j^Tx_i}}{\sum_{l=1}^ke^{\theta_l^Tx_i}}
$$

### 26、解释一下交叉熵**

### 27、交叉熵函数的形式*

$$
-\frac{1}{m}\sum_{j=1}^{m}\sum_{i=1}^{k}t_{ji}logP_{ji}
$$

$$
-\frac{1}{m}\frac{1}{k}\sum_{j=1}^{m}\sum_{i=1}^k[t_{ji}logP_{ji}+(1-t_{ji})log(1-P_{ji})]
$$

$$
-\frac{1}{m}\sum_{j=1}^m[t_jlogP_j+(1-t_j)log(1-P_j)]
$$

(1) 式子1，用于那些类别之间互斥(如：一张图片中只能保护猫或者狗的其中一个)的单任务分类中。连接的 softmax层之后的概率分布。

(2) 式子2，用于那些类别之间不存在互斥关系(如:一张图片中可有猫和狗两种以上的类别同时存在)的多任务学习分类中。最后一层的每个节点不在是softmax函数的输出了，而是sigmoid。把每个节点当成一个完整的分布，而式子1是所有节点组合程一个完整分布。

(3) 式子3，用于最后一层只有一个节点的二分类任务。

### 28、为什么用交叉熵损失函数而不用均方误差函数？**

### 29、在哪些场景下的分类问题不适用交叉熵损失函数？*

对于交叉熵损失函数的原始形式：
$$
-\frac{1}{m}\sum_{j=1}^{m}\sum_{i=1}^{k}t_{ji}logP_{ji}\tag{1}
$$
(1)式默认每个样本有且只有一个分类标记，因此只适用于单标记分类问题，也就是神经网络最后一层是softmax的情况。对于多标记分类问题，不能直接使用交叉熵损失函数的原始形式。需要变换成如下形式：
$$
-\frac{1}{m}\sum_{j=1}^{m}\sum_{i=1}^k[t_{ji}logP_{ji}+(1-t_{ji})log(1-P_{ji})]
$$

### 30、最大似然和交叉熵**

### 31、极大似然函数和极大后验函数是啥？*

### 32、回归问题的损失函数都有哪些？从哪些角度设计一个合理的损失函数？*

损失函数参见问题6。

设计损失函数的考虑：

(1) 模型在实际场景中的侧重点。比如重查准率还是查全率，像辅助诊断癌症的模型就要求有高查全率，漏查是需要极力避免的情况。通过这个我们可以设计比较合理的正负样例权重。

(2) 函数的数学性质：

a、是否可导

一般情况下模型的优化都需要用到梯度下降，所以可求导是一个损失函数必备的条件。对于XGBoost这种用到二阶导数信息的模型来说，要求函数二阶可导。 

b、是否为凸函数

这一点不是必须的，但如果目标函数是凸函数将有助于求得全局最优解。像LR模型的目标函数关于w为凸函数，但如果损失函数换成均方误差，则模型非凸。

### 33、LR怎么实现多分类？*

(1) one vs rest： 每个类别单独训练一个分类器；

(2) 修改损失函数，扩展为softmax回归；

### 34、SVM推导*\*\*\*\*\*\*\*\*\*\*\*

### 35、SVM的损失函数*\*\*\*

### 36、SVM核函数，对核函数有什么要求？本质是什么？*\*\*\*\*

### 37、SVM如何选择核函数？*\*

### 38、SVM如何进行多分类，为什么一般不这么做？*\*

### 39、SVM能否通过梯度下降来求解？*

### 40、LR和SVM的区别*\*\*\*\*\*\*\*\*\*\*\*\*

