[TOC]

## 目标检测

### 0、mAP评估指标？

mAP是每个类别AP的平均值，而AP是PR曲线下的面积。

目标检测中，TP是IOU>0.5的检测框数量，FP是IOU<0.5的检测框和对同一个GT多余的检测框数量。FN是没有被检测出来的GT的数量。

### 1、IOU是什么？用代码写一个IOU函数

IOU的意思是是交并比。

<img src="F:/clone/InterviewExperienceCollection/KnowledgePoint/iou.png" width="400px" align=center/>

它的公式可以表示为：
$$
IOU=\dfrac{A\cap B}{A\cup B}
$$

```python
#并集
def union(au, bu, area_intersection):
    area_a = (au[2] - au[0]) * (au[3] - au[1])
    area_b = (bu[2] - bu[0]) * (bu[3] - bu[1])
    area_union = area_a + area_b - area_intersection
    return area_union

#交集
def intersection(ai, bi):
    x = max(ai[0], bi[0])
    y = max(ai[1], bi[1])
    w = min(ai[2], bi[2]) - x
    h = min(ai[3], bi[3]) - y
    if w < 0 or h < 0:
        return 0
    return w*h

#交并比
def iou(a, b):
    # a and b should be (x1,y1,x2,y2)
    if a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:
        return 0.0
    area_i = intersection(a, b)
    area_u = union(a, b, area_i)
    return float(area_i) / float(area_u + 1e-6)
```

### 2、RCNN

1、利用选择性搜索算法从图像中提取2000个左右的region proposal；

2、把这些region proposal缩放成227*227的大小并用5层卷积两层全连接的CNN提取特征；

3、用SVM训练CNN提取出来的特征；

4、对SVM分好类的region proposal做bbox regression，用回归值来矫正建议窗口，生成预测的窗口坐标。

缺点：

(1)     训练分为多个阶段，步骤繁琐：微调网络+训练SVM+训练边框回归器；

(2)     训练耗时，占用磁盘空间大；5000张图像产生几百G的特征文件；

(3)     速度慢：使用GPU，VGG16模型处理一张图像需要47s；

(4)     测试速度慢：每个候选区域需要运行整个前向CNN计算；

(5)     SVM和回归是事后操作，在SVM和回归过程中CNN特征没有被学习更新。

### 3、SPP Net

一句话来说就是用SPP layer从不同大小的窗口提取出同样长度的特征向量。之后fast rcnn的ROI层就是借鉴了SPP Net的这种思路。

一张图片放入CNN网络中经过多层转化后，会生成多张特征图：

<img src="F:/clone/InterviewExperienceCollection/KnowledgePoint/spp.png" width="600px" align=center/>

而在原图中任意一个窗口在这些特征图中都有对应的窗口，SPP就是在这些对应的窗口中做池化操作使每张特征图都生成21维的特征向量。

具体做法是，在conv5层得到的特征图是256层，每层都做一次spatial pyramid pooling。先把每个特征图分割成多个不同尺寸的网格，比如网格分别为4\*4、2\*2、1\*1,然后每个网格做max pooling，这样256层特征图就形成了16\*256，4\*256，1\*256维特征，他们连起来就形成了一个固定长度的特征向量，将这个向量输入到后面的全连接层。



### 4、Fast RCNN

1、利用selective search 算法在图像中从上到下提取2000个左右的建议窗口(Region Proposal)；

2、将整张图片输入CNN，进行特征提取；

3、把建议窗口映射到CNN的最后一层卷积feature map上；

4、通过RoI pooling层使每个建议窗口生成固定尺寸的feature map；

5、利用Softmax Loss(探测分类概率) 和Smooth L1 Loss(探测边框回归)对分类概率和边框回归(Bounding box regression)联合训练。

**相比R-CNN，主要两处不同:**

(1)最后一层卷积层后加了一个ROI pooling layer，**ROI是SPP Net的简化版本，SPP Net是对一个对应窗口进行4\*4，2\*2，1\*1的pooling，也就是说一个区域的对应特征图能出来21维的特征向量；而ROI layer只进行7\*7的池化，所以只会出来49维的特征向量**。

(2)损失函数使用了多任务损失函数(multi-task loss)，将边框回归直接加入到CNN网络中训练。

### 5、Faster RCNN

faster rcnn=fast rcnn+rpn层；

- #### RPN 

  RPN层的作用是选出一些候选区域供fast rcnn网络的训练和预测，起加速作用。**rpn的核心是anchor机制，即对于一个卷积后的锚点可以对应回原图的不同宽高比例，不同大小的区域。在faster rcnn中anchor数量设置为k=9。**

  <img src="F:/clone/InterviewExperienceCollection/KnowledgePoint/anchor.png" width="500px" align=center/>

  原图像经过一个卷积网络会得到H\*W\*256的feature map，在论文里的H*W是原图像的\dfrac{1}{16}，特征图可以理解为原图的一种缩放。**那么，对于每个anchor，有一个256维度的特征向量来描述。这个特征向量对应原图像中k个anchor**；接下来每个anchor的256维向量都要经过两个分支进行训练：一个分支是候选区域分类，256->2\*k，一共k组输出，每组分别表示目标和前景的概率；一个分支是bbox regression，256->4\*k，一共k组输出，每一组4维向量分别表示候选区域的位置和大小：(t_x,t_y,t_w,t_h)。

  其中，每个anchor是否为真实区域取决于IOU的大小，与任意一个真实区域的IOU>0.7则为标定为正样本，与所有真实区域的IOU<0.3则标定为负样本。

  RPN层和fast rcnn是共享特征图的，只不过RPN层是利用anchor机制事先选择得到候选框。这部分候选框映射到的特征图的区域经过ROI pooling得到相同大小的特征向量，传递给后面两个分支：fc层分类和回归。

### 6、Mask RCNN

mask rcnn是基于faster rcnn工作的基础上提出的同时可以进行目标检测和目标分割的模型，可以认为mask rcnn = faster rcnn+RoIAlign+mask分支。

- #### RoIAlign

  mask rcnn不仅需要检测候选框的类别，还需要给出目标类别的mask映射。mask映射应该遵循平移等价性，就是说框移动，输出的mask也会相应移动 (平移不变性是指框移动，输出也不会变化，比如分类器)。

  所以，传统的fast rcnn在这一点上有着较大的缺陷，因为它在卷积过程和ROI层经过了两次量化损失。比如说，原图像256\*256，而特征图是16\*16，缩小为原来的\dfrac{1}{16}。那么原图中坐标17映射到特征图上就是1.0625，小数部分会被舍去。而在ROI层，如果框的大小是15\*15，要进行7\*7的pooling，又是一次损失。

  <img src="F:/clone/InterviewExperienceCollection/KnowledgePoint/roialign.png" width="700px" align=center/>

  这种损失会导致生成的mask和目标无法对齐。所以，RoIAlign为了得到浮点数坐标的像素值，利用了双线性插值算法。 像上图种每个bin里面的四个点的像素值，都是由这种方法得到的，之后再进行正常的max pooling操作。

- #### mask rcnn structure

  mask rcnn和faster rcnn除了RoIAlign的区别外，另一个区别就是加了一个mask的分支，如下图：

  <img src="F:/clone/InterviewExperienceCollection/KnowledgePoint/maskrcnn.png" width="700px" align=center/>

  以ResNet为例：候选区域经过RoIAlign层得到7\*7*2048的feature map，mask分支是通过FCN网络来实现的，最终得到了14\*14\*80的输出，这里的80就是类别数，也就是有80个mask，但只有一个是对应于目标类别的。所以需要等另一个分支预测得到区域类别后选择对应的mask输出。

### 7、yolo

one-stage目标检测的主流方法，速度比rcnn系列模型要快很多，在一些注重速度的目标检测系统中经常会用到。

- #### [yolo v1](https://blog.csdn.net/hrsstudy/article/details/70305791)

  该框架假设一张448\*448的图片被划分成S\*S的网格 (论文中S=7)，经过如下的网络结构后输出了S\*S\*N的向量，每个N维向量对应于一个格子的预测信息。其中N=B*5+C，B是认为设置的要预测的bbox数量，C是目标类别数 (论文中分别是2和20)；所以N=30。

  <img src="F:/clone/InterviewExperienceCollection/KnowledgePoint/yolov1.png" width="800px" align=center/>

  这里记录几个关键点：

  - 其中bbox的位置信息是一个5维向量：(x,y,w,h)，类别预测向量是20维。坐标x,y代表了预测的bbox的中心与栅格边界的相对值。 坐标w,h代表了预测的bounding box的width、height相对于整幅图像width,height的比例。

  - **ground truth box只有一个，要同时预测多个bbox信息的目的是起到一种集成的作用。这些bbox位置大小信息的输出在训练过程中都是以同一个ground truth box的信息来计算loss的。**

  - 输出类别概率的公式为：
    $$
    \operatorname { Pr } \left( \text { Class } _ { i } | \text { Object } \right) * \operatorname { Pr } ( \text { Object } ) * \text {IOU}_{\text{pred}}^{\text{truth}} = \operatorname { Pr } \left( \text { Class } _ { i } \right) * \text{IOU}_{\text{pred}}^{\text{truth}}
    $$

  - 

  <img src="F:/clone/InterviewExperienceCollection/KnowledgePoint/yololossfunc.png" width="800px" align=center/>

  - 因为每个格子都会给出两个bbox的预测，所以在一张图片中每个目标类别会得到7\*7\*2个bbox和概率值。要得到最终的bbox还得经过一个后处理步骤：NMS (非极大值抑制)。

    <img src="F:/clone/InterviewExperienceCollection/KnowledgePoint/nms.png" width="600" align=center/>



- #### yolo v2


### 8、[SSD](https://zhuanlan.zhihu.com/p/31427288)

除了yolo，SSD是目标检测领域one-stage方法的另一主流框架，和yolo的区别如下：

1、yolo只用最高层的特征图 (7\*7\*1024)来进行预测，这样做速度很快但在检测小物体上就不尽人意。SSD效仿了特征金字塔的思路，**在6种尺度的特征图上进行softmax分类和bbox回归**；

2、SSD还加入了Prior box，类似于RPN的anchor；

- #### structure

  <img src="F:/clone/InterviewExperienceCollection/KnowledgePoint/ssd0.png" width="800" align=center/>



  **conv4_3**网络分为了3条线路:

  1、经过bn层+一次卷积后进行softmax分类，预测目标类别；因为特征图大小为38\*38\*512，每个位置对应于一个512维的特征向量，所以相当于进行38\*38次512维向量的softmax分类。

  2、经过bn层后+一次卷积后特征图每个点生成了4\*num_priorbox大小的特征，这些特征用于后面bbox regression。

  3、进行bbox regression，特征图每个点经过regressor生成2\*4\*num_priorbox维度的输出。每个prior box得到一组(x1,y1,x2,y2)和四个坐标值的方差预测，8个数。


- #### Prior box

  <img src="F:/clone/InterviewExperienceCollection/KnowledgePoint/priorbox.jpg" width="500" align=center/>

  SSD按照如下规则生成prior box：

  - 以feature map上每个点为中心，生成一系列同心的prior box

  - 正方形prior box最小边长和最大边长为：
    $$
    min\_size\\
    \sqrt{min\_size*max\_size}
    $$

  - 

  $$
  \sqrt{aspect\_ratio}*min\_size\\
  1/\sqrt{aspect\_ratio}*min\_size
  $$

  每个feature map对应的min_size和max_size由以下公式决定：

  - 对于conv4_3：k = 1，min_size = s1\*300，max_size = s2\*300
  - 对于conv-7：k = 2，min_size = s2\*300，max_size = s3\*300

### 9、Focal Loss

Focal loss是针对交叉熵在样本不平衡的场景下做出的改进，简单来说就是给负样本加了一个能随分类难度变化而变化的权重，从而让易分类的样本对loss的贡献变小，让难分类样本和数量少的类别对loss的贡献变大。
$$
CE(p,y)=CE(p_t)=-log\ p_t\\
\text{subject to: } p_t=\left\{\begin{array}{cc} 
p,& if\ \ y=1\\ 
1-p,& otherwise 
\end{array}\right.
$$
上面的公式是交叉熵损失 (对于类别t的loss)，是目标检测中常用的损失函数。对于two-stage方法而言，这个损失函数并没有什么不足因为有RPN进行初步的候选区域采样。所以对于后面的网络，正负样本比例不会有严重失衡的情况。但**对于one-stage方法来说，采样得到的样本框大部分都是背景，这种情况下负样本占了loss的大部分贡献，会影响正样本的分类性能。**

一个直观的方法是在样本少的类别前面加上一个大于1的权重:
$$
CE(p_t)=-\alpha_tlog\ p_t
$$
但这种办法只能解决样本少的类别问题，并没有解决样本分类难易的问题。因此有了如下的形式：
$$
FL(p_t)=-(1-p_t)^{\gamma}log\ p_t
$$
对于预测概率p_t高的样本，FL loss认为它是容易分类的样本，因此给的权重就比较低。而对于预测概率比较低的样本，FL loss认为它们难以分类，给的权重也比较高；所以模型会侧重训练这些样本。综合以上两种形式可得：
$$
FL(p_t)=-\alpha_t(1-p_t)^{\gamma}log\ p_t
$$


### 10、FPN

feature pyramid networks (特征金字塔网络) 的提出是为了解决图像的多尺度目标检测问题。在目标检测中，一个目标类别可能有很多种尺度，比如在街头，远处的行人和近处的行人尺度就相差很大。这种多尺度问题用传统网络来做精度不够令人满意。因为目标图像再经过深层卷积网络后，语义信息增强但位置信息减弱，所以最终检测效果会受到影响。一个简单的思路是把目标图像进行多尺度缩放，一起交给模型进行训练。显然这种方法十分耗费计算资源和训练时间。所以作者希望提出一种网络结构，让图像在经过网络后既能提取出深层的语义信息，也能保留原图的位置信息。

图像在经过一些卷积层和池化层后，一般都是通道数变多，特征图变小。参见resnet：

<img src="F:/clone/InterviewExperienceCollection/KnowledgePoint/resnet.png" width="800px" align=center/>

可以看出，卷积层[C2,C3,C4,C5]特征图每次长宽缩小为原来的\dfrac{1}{2}。**这个过程可以看成金字塔结构由底向上走，图像的坐标信息逐渐减弱，但语义信息随着通道数的增加而增强。**这就是FPN的第一阶段，金字塔向上走的过程。

<img src="F:/clone/InterviewExperienceCollection/KnowledgePoint/fpn1.jpg" width="700px" align=center/>

接下来是FPN的第二阶段：自顶向下——深层的特征图和浅层特征图的融合。如下图所示，**深层的特征图通过上采样变换成原来的2倍，下一层的特征图进行1\*1卷积增加通道数，两层相加，逐层往下走。这个过程使图像的深层语义信息得以保留，并且引入了底层的位置信息。到了底部后，再进行一次3\*3的卷积，去除上采样的混叠现象。**

<img src="F:/clone/InterviewExperienceCollection/KnowledgePoint/fpn2.jpg" width="700px" align=center/>

